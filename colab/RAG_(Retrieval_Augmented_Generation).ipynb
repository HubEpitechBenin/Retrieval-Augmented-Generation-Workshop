{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "har7eMYHMjOq"
      },
      "source": [
        "# ***INTELLIGENT CHATBOT SYSTEM ðŸ§  BASED ON RAG FOR KNOWLEDGE EXTRACTION FROM PDF DOCUMENTS***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2ML_DYDJlxr"
      },
      "source": [
        "## **STEP 1:** ENVIRONMENT SETUP\n",
        "\n",
        "In this section, we install all the necessary dependencies for our RAG implementation:\n",
        " - **langchain**: Framework for developing applications with Large Language Models\n",
        " - **faiss-cpu**: Facebook AI Similarity Search for efficient vector similarity search\n",
        " - **openai**: For interacting with OpenAI models (or compatible APIs)\n",
        " - **langchain integrations**: To connect with various model providers\n",
        " - **sentence-transformers**: For generating embeddings from text\n",
        " - **pypdf**: For processing and extracting content from PDF documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "stJMjgVC-Hi3",
        "outputId": "ac725f28-b3dc-4481-e8a2-f35021056be1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.23)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (1.10.0)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.70.0)\n",
            "Requirement already satisfied: langchain_openai in /usr/local/lib/python3.11/dist-packages (0.3.12)\n",
            "Requirement already satisfied: langchain_community in /usr/local/lib/python3.11/dist-packages (0.3.21)\n",
            "Collecting langchain_huggingface\n",
            "  Downloading langchain_huggingface-0.1.2-py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (3.4.1)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.30.1)\n",
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.11/dist-packages (3.0.1)\n",
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.11/dist-packages (5.4.0)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.51 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.51)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.24)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.40)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.13.1)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.11/dist-packages (from langchain_openai) (0.9.0)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (3.11.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (9.1.2)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.8.1)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.4.0)\n",
            "Requirement already satisfied: tokenizers>=0.19.1 in /usr/local/lib/python3.11/dist-packages (from langchain_huggingface) (0.21.1)\n",
            "Requirement already satisfied: transformers>=4.39.0 in /usr/local/lib/python3.11/dist-packages (from langchain_huggingface) (4.50.3)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.14.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2025.3.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.3.2)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.18.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.51->langchain) (1.33)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.16)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (1.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.3.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2024.11.6)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.39.0->langchain_huggingface) (0.5.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.51->langchain) (3.0.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Downloading langchain_huggingface-0.1.2-py3-none-any.whl (21 kB)\n",
            "Installing collected packages: langchain_huggingface\n",
            "Successfully installed langchain_huggingface-0.1.2\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain faiss-cpu openai langchain_openai langchain_community langchain_huggingface sentence-transformers huggingface_hub pypdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2C8sRIzoJ3WY"
      },
      "source": [
        "### Authentication with Hugging Face\n",
        "\n",
        "Connect to Hugging Face Hub to access embedding models and other AI resources. Replace the placeholder with your actual HF token."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZcZe0hQAAJY-"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "HF_TOKEN = \"****************************\"\n",
        "login(token=HF_TOKEN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izL2Dr-tJ8hP"
      },
      "source": [
        "## **STEP 2:** DATA INGESTION AND PREPROCESSING\n",
        "\n",
        "This is a critical step in any RAG system where we:\n",
        "1. Load the source documents (PDFs in this case)\n",
        "2. Split the document into manageable chunks\n",
        "\n",
        "The chunk size (10000 characters) and overlap (200 characters) are important parameters:\n",
        "- **Larger chunks** preserve more context but can lead to less relevant retrievals\n",
        "- **Chunk overlap** helps maintain continuity between chunks\n",
        "- These parameters should be adjusted based on document type and query patterns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6R1jx1lu_P0Z"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "loader = PyPDFLoader(\"B-CNA-500-my_torch.pdf\")\n",
        "pages = loader.load()\n",
        "\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=10000, chunk_overlap=200)\n",
        "chunks = splitter.split_documents(pages)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQMsuAtfKYkS"
      },
      "source": [
        "## **STEP 3:** VECTOR STORE CREATION\n",
        "\n",
        "This section transforms document chunks into vector embeddings and stores them for efficient similarity search. Key components:\n",
        "\n",
        "1. **Embedding Model**: all-MiniLM-L6-v2 is chosen for its balance between performance and efficiency\n",
        "2. **FAISS Vector Database**: Enables fast similarity search at scale\n",
        "\n",
        "Alternative embedding models to consider based on performance needs:\n",
        "  - **Small models**: faster but less accurate (e.g., all-MiniLM-L6-v2)\n",
        "  - **Large models**: more accurate but slower (e.g., all-mpnet-base-v2)\n",
        "  - **Domain-specific models**: tuned for specific content types"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UucSewW5BCJP"
      },
      "outputs": [],
      "source": [
        "from langchain.vectorstores import FAISS\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "        model_name=\"all-MiniLM-L6-v2\",\n",
        "        model_kwargs={\"device\": \"cpu\"},\n",
        "        encode_kwargs={\"normalize_embeddings\": True}\n",
        ")\n",
        "vectorstore = FAISS.from_documents(chunks, embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGAvGi2WK4VW"
      },
      "source": [
        "## **STEP 4:** LLM SELECTION AND RAG CHAIN CONFIGURATION\n",
        "\n",
        "In this step we:\n",
        "1. Configure the LLM that will generate responses (DeepSeek in this case)\n",
        "2. Set up the RAG retrieval chain that connects:\n",
        "    - The vector store (retrieval component)\n",
        "    - The LLM (generation component)\n",
        "\n",
        "### Important LLM Parameters:\n",
        "- **temperature=1.3**: Relatively high setting that encourages creative responses (lower values like 0.3 would produce more deterministic and factual results)\n",
        "- **max_tokens=500**: Limits the length of the response\n",
        "\n",
        "### Consider experimenting with:\n",
        "- Different retrieval strategies (e.g., MMR, SelfQueryRetriever)\n",
        "- Chain types (stuff, map_reduce, refine, map_rerank)\n",
        "- Search parameters (k, fetch_k, lambda_mult)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ewXmrt2yB4dL"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "# Configuration du LLM (DeepSeek dans ce cas)\n",
        "DEEPSEEK_API_KEY = \"sk-*************************\"\n",
        "DEEPSEEK_API_BASE = \"https://api.deepseek.com/v1\"\n",
        "\n",
        "def get_deepseek_llm():\n",
        "    return ChatOpenAI(\n",
        "        model=\"deepseek-chat\",\n",
        "        openai_api_key=DEEPSEEK_API_KEY,\n",
        "        openai_api_base=DEEPSEEK_API_BASE,\n",
        "        temperature=1.3,\n",
        "        max_tokens=500\n",
        "    )\n",
        "\n",
        "llm = get_deepseek_llm()\n",
        "\n",
        "# CrÃ©ation du pipeline RAG utilisant la chaÃ®ne de questions-rÃ©ponses avec rÃ©cupÃ©ration\n",
        "qa_chain = RetrievalQA.from_chain_type(llm=llm, retriever=vectorstore.as_retriever())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9voiXwIYMIzG"
      },
      "source": [
        "## **STEP 5:** QUERY EXECUTION AND RESPONSE GENERATION\n",
        "\n",
        "In this final step, we:\n",
        "1. Take a user query\n",
        "2. Retrieve relevant context from our vector store\n",
        "3. Generate a response using the LLM enriched with the retrieved context\n",
        "\n",
        "This is where the magic of RAG happens - the LLM's response is now grounded in the specific content of your document, rather than just its general pre-training.\n",
        "\n",
        "### Production Considerations:\n",
        "- Add evaluation metrics to measure relevance and accuracy\n",
        "- Implement user feedback loops to improve retrieval quality\n",
        "- Add caching for frequent queries\n",
        "- Monitor token usage and response times\n",
        "- Implement streaming responses for better user experience"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8yZ6TKQBCN55",
        "outputId": "89a7a0e4-9043-4b30-e244-afc01add1e07"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The goals of the project are to deliver two binaries:\n",
            "\n",
            "1. **Neural Network Generator**:  \n",
            "   - Generates a new neural network from a configuration file.  \n",
            "   - Must be implemented from scratch (libraries like PyTorch or TensorFlow are **not** allowed).  \n",
            "\n",
            "2. **Chessboard Analyzer**:  \n",
            "   - Can be launched in **training mode** (to train the neural network) or **evaluation mode** (to analyze chessboards).  \n",
            "   - Must use **supervised learning** for training.  \n",
            "   - Requires a pre-trained neural network (named `my_torch_network*`).  \n",
            "\n",
            "### Additional Requirements:  \n",
            "- Provide **documentation** (README, benchmarks, justification of design choices).  \n",
            "- Keep all **scripts and training datasets** used for reproducibility.  \n",
            "- Error messages must be written to **stderr**, and the program should exit with code **84** on errors (**0** if successful).  \n",
            "\n",
            "### Bonus Options (Optional Enhancements):  \n",
            "- Optimize training speed using **parallel computing** (multithreading, GPGPU, etc.).  \n",
            "- Display **multiple learning curves** for comparison.  \n",
            "- Evaluate learning with **multiple metrics**.  \n",
            "- Implement a **full chess AI**.  \n",
            "\n",
            "### Final Notes:  \n",
            "- The project simulates chess-based strategic decisions for an army.  \n",
            "- The solution must be **machine-learning-based** (no hardcoded chess logic unless in a bonus).  \n",
            "- Pre-trained models and training data must be available for demonstration during the defense.  \n",
            "\n",
            "Would you like clarification on any specific part?\n"
          ]
        }
      ],
      "source": [
        "query = \"What are the goals of the project?\"\n",
        "response = qa_chain.run(query)\n",
        "print(response)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
